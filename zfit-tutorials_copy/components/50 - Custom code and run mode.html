
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Custom Code with zfit &#8212; zfit</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../../_static/zfit-favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      <h1 class="site-logo" id="site-title">zfit</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../pages/5_minutes_to_zfit.html">
   5 minutes to zfit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/Introduction.html">
   Introduction to zfit
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../TensorFlow/HPC_with_TensorFlow.html">
   HPC in Python with TensorFlow
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="20%20-%20Composite%20Models.html">
   Composite Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="60%20-%20Custom%20PDF.html">
   Creating your own pdf
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../guides/custom_models.html">
   Custom models
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/zfit-tutorials_copy/components/50 - Custom code and run mode.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/zfit/poster"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/zfit/poster/issues/new?title=Issue%20on%20page%20%2Fzfit-tutorials_copy/components/50 - Custom code and run mode.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/zfit/poster/edit/master/website/zfit-tutorials_copy/components/50 - Custom code and run mode.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/zfit/poster/master?urlpath=lab/tree/website/zfit-tutorials_copy/components/50 - Custom code and run mode.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/zfit/poster/blob/master/website/zfit-tutorials_copy/components/50 - Custom code and run mode.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-computations">
   Defining computations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-vs-eager">
   Graph vs Eager
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eager">
     Eager
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph">
     Graph
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-restrictions">
     Graph restrictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trigger-a-graph-build">
     Trigger a graph build
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-modes-in-zfit">
     Graph modes in zfit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#python-code-in-graph">
     Python code in graph
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradients-in-tensorflow">
   Gradients in TensorFlow
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resetting">
   Resetting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#caching-and-slowness">
     Caching and slowness
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#advantages-of-graphs">
   Advantages of graphs
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="custom-code-with-zfit">
<h1>Custom Code with zfit<a class="headerlink" href="#custom-code-with-zfit" title="Permalink to this headline">¶</a></h1>
<p>zfit provides a lot of opportunities to implement your own code, be it in a model, loss or similar. It is mainly based on TensorFlow for it’s computations, which offers many advantages in terms of performance. While this is similar in nature to Numpy, some things are maybe not possible to implement and a fallback can be done.</p>
<p>This tutorial introduces the concept of TensorFlow and how to use which mode in zfit.</p>
<p><strong>TL;DR</strong>: for immediate advice on what to use, also checkout the <a class="reference external" href="https://github.com/zfit/zfit/wiki/FAQ#graph-and-gradient-modes">FAQ on graph and gradient modes</a></p>
<div class="section" id="defining-computations">
<h2>Defining computations<a class="headerlink" href="#defining-computations" title="Permalink to this headline">¶</a></h2>
<p>Python offers a lot of flexibility as a programming language. In model fitting, much of this flexibility is though not needed. Fitting using a NLL with a dataset and a Gaussian is a straightforward process: a mathematical definition of a computation that is repeated many times, varying the parameters until the minimum is found.
The expression always stays the same. But in fact, we could split things into two stages:</p>
<ul class="simple">
<li><p>defining the computation: we build our model, whether it’s a Gaussian or something more complicated by sticking together models. Creating the NLL with the data and the model results in a well defined expression. From now on, there is no need to change this expression (in the vast majority of cases) and it can be ‘cached’</p></li>
<li><p>running: using our definition, we now need to do the actual calculation. Change the parameters, and calculate again. In this step, we do not need to redefine the computational expression, this stays the same. Just the values change.</p></li>
</ul>
<p><em>If</em> we can split our process into this two parts (and we mostly can), the first part has to be run only once, can be optimized (even using many resources as it is a one-time process) and then it can be executed many times.</p>
<p>On the other hand, we may have completely dynamic things, a model that changes it’s components after every minimization. While unlikely, let’s assume there is a case. Then we would need to rerun the model building every time, which is rather costly but this is the inherent price of it.</p>
</div>
<div class="section" id="graph-vs-eager">
<h2>Graph vs Eager<a class="headerlink" href="#graph-vs-eager" title="Permalink to this headline">¶</a></h2>
<div class="section" id="eager">
<h3>Eager<a class="headerlink" href="#eager" title="Permalink to this headline">¶</a></h3>
<p>Using Numpy does the latter; it does not remember the previous executions, but simply computes what is given on every line of code, even if done n times. This is also called “eager”. It is the “normal” behavior we are used to from Python.</p>
<p>TensorFlow by default acts exactly the same as Numpy and there is merely a difference to spot. In fact, Tensors can direclty be given to Numpy functions (<strong>in eager execution</strong>). So far, they are similar.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;ZFIT_DISABLE_TF_WARNINGS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>  <span class="c1"># disables some TF warnings</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">zfit</span>
<span class="kn">from</span> <span class="nn">zfit</span> <span class="kn">import</span> <span class="n">z</span>  <span class="c1"># this is basically tf, just wrapped</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnd_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,))</span>
<span class="n">rnd_tf</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rnd_np</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rnd_tf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.44194106  0.55505251  0.98471163 -1.42439679  1.0426848 ]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor([-0.40727824  0.1405432  -1.23463046 -0.88077355 -0.13073378], shape=(5,), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnd_sum</span> <span class="o">=</span> <span class="n">rnd_np</span> <span class="o">+</span> <span class="n">rnd_tf</span>  <span class="c1"># this will be a tf.Tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rnd_sum</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor([-0.8492193   0.69559571 -0.24991883 -2.30517035  0.91195103], shape=(5,), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">rnd_sum</span><span class="p">)</span>  <span class="c1"># this will be a numpy nd.array</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.72117342, 0.48385339, 0.06245942, 5.31381034, 0.83165467])
</pre></div>
</div>
</div>
</div>
<p>As we see, this two libraries can, so far, be quite mixed. Or: TensorFlow can act “numpy-like” as a subset of it’s functionality. But it offers way more:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eager_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;I am Python, being executed, x is &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;I am TensorFlow, being executed, x is &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eager_func</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am Python, being executed, x is 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(5.0, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am TensorFlow, being executed, x is  5
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(), dtype=float64, numpy=25.0&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eager_func</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">7</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am Python, being executed, x is 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(7.0, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am TensorFlow, being executed, x is  7
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(), dtype=float64, numpy=49.0&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="graph">
<h3>Graph<a class="headerlink" href="#graph" title="Permalink to this headline">¶</a></h3>
<p>TensorFlow has the possibility to decorate a function with <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> (or here, use <code class="docutils literal notranslate"><span class="pre">z.function</span></code>!). This will first go through the code, stick together a computational graph (= build the computational expression) and then execute it directly. If “similar” arguments are given (e.g. just different data), it will actually re-use the computational expression. Building this graph in the first place has many advantages, as will be seen later.</p>
</div>
<div class="section" id="graph-restrictions">
<h3>Graph restrictions<a class="headerlink" href="#graph-restrictions" title="Permalink to this headline">¶</a></h3>
<p>If a graph is built, every <code class="docutils literal notranslate"><span class="pre">tf.*</span></code> operation is recorded and remembered as a computation. On the other hand, any Python code is executed just once (not guaranteed, maybe also 2-3 times, but not more &lt;- advanced implementation detail). So numpy operations <em>won’t</em> be in the computational graph and are treated as constants or will fail if they want to do something on a Tensor. Because the graph building will use a symbolic Tensor (as opposed to above where we had eager Tensors.</p>
<p>Creating the above function but decorater will demonstrate this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@z</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">graph_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;I am Python, being executed, x is &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;I am TensorFlow, being executed, x is &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">graph_func</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am Python, being executed, x is 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensor(&quot;x:0&quot;, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am TensorFlow, being executed, x is  5
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(), dtype=float64, numpy=25.0&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">graph_func</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">7</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am TensorFlow, being executed, x is  7
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(), dtype=float64, numpy=49.0&gt;
</pre></div>
</div>
</div>
</div>
<p>There are two main differences here compared to the <code class="docutils literal notranslate"><span class="pre">eager_func</span></code>:</p>
<ul class="simple">
<li><p>The Python print statement sais that x is a Tensor. But now, there is no value associated with. This is because we now look at a symbolic Tensor, a special object known by TensorFlow only. This will later on have a value, but only then.</p></li>
<li><p>In the second run with 7, the Python print statement vanished! This is because there is <em>only</em> the graph run again. In the graph, only <code class="docutils literal notranslate"><span class="pre">tf.*</span></code> operations (<code class="docutils literal notranslate"><span class="pre">tf.print</span></code>, <code class="docutils literal notranslate"><span class="pre">tf.square</span></code>) are added, no Python operation.</p></li>
</ul>
<p>Let’s see more explicitly about the first point and how it will fail by using a Numpy operation instead of TF</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@z</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">graph_func_fail</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;I am Python, being executed, x is &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;I am TensorFlow, being executed, x is &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">graph_func_fail</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">5.</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">NotImplementedError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error was raised, last line: </span><span class="si">{</span><span class="n">error</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am Python, being executed, x is 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensor(&quot;x:0&quot;, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Error was raised, last line: Cannot convert a symbolic Tensor (x:0) to a numpy array. This error may indicate that you&#39;re trying to pass a Tensor to a NumPy call, which is not supported
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>The error message is clear: Numpy does not know how to deal with a symbolic Tensor since it does not know the symbolic language of TensorFlow. It cab only act on concrete numbers given.</p>
<p>So far we have seen: when building a graph, we can only use <code class="docutils literal notranslate"><span class="pre">tf.*</span></code> acting on inputs.</p>
</div>
<div class="section" id="trigger-a-graph-build">
<h3>Trigger a graph build<a class="headerlink" href="#trigger-a-graph-build" title="Permalink to this headline">¶</a></h3>
<p>Above we used directly Tensors to feed into a function. This is usually the most efficient way, as any arbitrary Python object can also be used, but (usually) causes a re-build of the graph, resp. adds a new graph into the function cache.</p>
<p>zfit has some internal logic to mitigate this <em>somewhat</em> and invalidate a graph when an object has changed.</p>
<p>However, calling the above with pure Python numbers will create a new graph every time (except it is exactly the same Python number). This is usually not what we want, but sometimes it is unavoidable. More on this further down.</p>
</div>
<div class="section" id="graph-modes-in-zfit">
<h3>Graph modes in zfit<a class="headerlink" href="#graph-modes-in-zfit" title="Permalink to this headline">¶</a></h3>
<p>Most functions in zfit build a graph first, most notably the loss (which will also build a graph in every model). This behavior can be changed with <code class="docutils literal notranslate"><span class="pre">zfit.run.set_mode(graph=False</span></code>), which will run functions eagerly. With this, our previously failed example should run.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zfit</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">set_mode</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">graph_func_fail</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am Python, being executed, x is 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(5.0, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I am TensorFlow, being executed, x is  5
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>25.0
</pre></div>
</div>
</div>
</div>
<p>This can be useful for debugging, as now every Tensor has a value and every operation is executed immediately.</p>
<p>Another problem is that building a graph becomes only efficient if we execute it multiple times, not just a single time. But for plotting a pdf for example, just a single call to <code class="docutils literal notranslate"><span class="pre">pdf</span></code> is needed.
Furthermore, since different objects (e.g. different datasets, <code class="docutils literal notranslate"><span class="pre">norm_range</span></code> etc.) will create a new graph, things can become very slow, caching many graphs that often are not needed anymore.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">z.function</span></code> decorator is in fact more powerful then the pure <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>: it allow to tell what kind of function is wrapped and this on the other hand allows zfit to be “smart” about which function to trace and which not. By default, any method of models (<code class="docutils literal notranslate"><span class="pre">pdf</span></code>, <code class="docutils literal notranslate"><span class="pre">integrate</span></code>,…) are executed eagerly, without graphs. On the other hand, if a loss is built, this builds a graph of everything. Mainly, this behavior is wanted.</p>
<p>It implies though that if a loss is built, the execution is different then as opposed to calling <code class="docutils literal notranslate"><span class="pre">pdf</span></code>, because the former will do graph tracing, then execute this graph, while the latter will execute eagerly (by default).</p>
<p>Therefore, it can also be beneficial to set <code class="docutils literal notranslate"><span class="pre">zfit.run.set_mode(graph=True)</span></code>, which will always trigger a graph tracing for any decorated function.</p>
</div>
<div class="section" id="python-code-in-graph">
<h3>Python code in graph<a class="headerlink" href="#python-code-in-graph" title="Permalink to this headline">¶</a></h3>
<p>So far, to execute arbitrary Python code, including Numpy, also in losses, we will need to run zfit eagerly (with <code class="docutils literal notranslate"><span class="pre">graph=False</span></code>). There is another possibility, which is the <code class="docutils literal notranslate"><span class="pre">z.py_function</span></code> (wrapping <code class="docutils literal notranslate"><span class="pre">tf.py_function</span></code>). This allows to wrap an “arbitrary” Python function and put it into the graph; the only restriction is that it allows Tensors as inputs and outputs (resp. Numpy arrays) only.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">numpy_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">a</span>

<span class="nd">@z</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">wrapped_numpy_func</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">,</span> <span class="n">a_tensor</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">py_function</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">numpy_func</span><span class="p">,</span> <span class="n">inp</span><span class="o">=</span><span class="p">[</span><span class="n">x_tensor</span><span class="p">,</span> <span class="n">a_tensor</span><span class="p">],</span> <span class="n">Tout</span><span class="o">=</span><span class="n">zfit</span><span class="o">.</span><span class="n">ztypes</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>  <span class="c1"># or tf.float64</span>
    <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">x_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># this is useful and can prevent bugs: it says that the shape of the</span>
    <span class="c1"># result is the same as of the input tensor. This does not have to be true always and may be adjusted</span>
    <span class="c1"># accordingly. It however prevents some failures e.g. related to sampling.</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>  <span class="c1"># we can of course continue to execute more tf operations</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wrapped_numpy_func</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,)),</span> <span class="n">z</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=
array([4.06786252, 2.56856789, 3.76112812, 4.52642042, 3.97926043,
       4.8008461 , 5.61824806, 2.72370821, 3.13417161, 0.2794445 ])&gt;
</pre></div>
</div>
</div>
</div>
<p>That’s nice! Now we can execute Python code <em>and</em> use the graphs. There is though a drawback: This computations are completely not optimized: they won’t be run on the GPU, parallelized or anything. Most notably, not using pure <code class="docutils literal notranslate"><span class="pre">z.*</span></code> (or <code class="docutils literal notranslate"><span class="pre">tf.*</span></code>) functionality has another implication: TensorFlow is not able to have a full computational expression, but there are unknowns, which makes another feature unusable: automatic gradients.</p>
</div>
</div>
<div class="section" id="gradients-in-tensorflow">
<h2>Gradients in TensorFlow<a class="headerlink" href="#gradients-in-tensorflow" title="Permalink to this headline">¶</a></h2>
<p>Tracking every operation that is done on a Tensor, it is possible to get an expression for the analytic gradient - by successively applying the chain rule to every operation. This technique is also called <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a>.</p>
<p>This is only possible if all operations are performed by TensorFlow, whether it is run eagerly or within a graph creating function.</p>
<p>If Numpy is used directly with TensorFlow in a dynamic way (e.g. not just a static shape), such as when using SciPy distributions, this gradient won’t work anymore. zfit can switch to a numerical method for calculating the gradient and Hessian with <code class="docutils literal notranslate"><span class="pre">zfit.run.set_mode(autograd=False)</span></code>. Futhermore, some optimizers such as <code class="docutils literal notranslate"><span class="pre">Minuit</span></code> have their own, iternal gradient calculator, which can be more efficient (<code class="docutils literal notranslate"><span class="pre">Minuit(use_minuit_grad=True)</span></code>).</p>
<p>Numerical gradients (provided by zfit) are less stable and tend to break.</p>
</div>
<div class="section" id="resetting">
<h2>Resetting<a class="headerlink" href="#resetting" title="Permalink to this headline">¶</a></h2>
<p>To reset to the default behavior, use <code class="docutils literal notranslate"><span class="pre">zfit.run.set_mode_default</span></code>.</p>
<div class="section" id="caching-and-slowness">
<h3>Caching and slowness<a class="headerlink" href="#caching-and-slowness" title="Permalink to this headline">¶</a></h3>
<p>Repeated calls to graph building functions are sometimes necessary, e.g. when scanning over a range and changing the <code class="docutils literal notranslate"><span class="pre">norm_range</span></code>, which renders the validity of the graph invalid. We can have a look at an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@z</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">graph_func2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">*=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">4</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.3</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>First, we can test this in eager mode to get an approximate idea</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_evals</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># how often to evaluate a function, e.g. the loss</span>
<span class="n">n_changes</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># how often the loss changes fundamentally and has to be rebuilt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it -n1 -r1
<span class="n">zfit</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">set_mode</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># running in eager mode</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_evals</span><span class="p">):</span>
        <span class="n">graph_func2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>13.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it -n1 -r1
<span class="n">zfit</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">set_mode</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># running in graph mode</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_evals</span><span class="p">):</span>
        <span class="n">graph_func2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>14.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it -n1 -r1
<span class="n">zfit</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">clear_graph_cache</span><span class="p">()</span>
<span class="n">zfit</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">set_mode</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># running in graph mode but clearing unused caches</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">zfit</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">clear_graph_cache</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_evals</span><span class="p">):</span>
        <span class="n">graph_func2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9.87 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zfit</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">set_mode_default</span><span class="p">()</span> <span class="c1"># resetting the mode</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;zfit.util.temporary.TemporarilySet at 0x7f0a3843ea60&gt;
</pre></div>
</div>
</div>
</div>
<p>We see that for this (simple) example, eager and graph building with cache cleared in between match basically. For more <code class="docutils literal notranslate"><span class="pre">n_evals</span></code>, the graph version with clearing will be more efficient, for less, the eager mode.
Building graphs and not clearing them will fill up the cache and significanlty slow things down, as demonstrated.</p>
</div>
</div>
<div class="section" id="advantages-of-graphs">
<h2>Advantages of graphs<a class="headerlink" href="#advantages-of-graphs" title="Permalink to this headline">¶</a></h2>
<p>The main advantages are the optimized execution including parallelization and dispatching to the GPU. Furthermore, many things such as operation fusions to an optimized implementation, constant folding and more is performed.</p>
<p>The performance gain is mostly visible with highly parallizable functions, such as building a sum. Let’s look at an example here, using the previous examples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@z</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">sum_func</span><span class="p">():</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">graph_func2</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">z</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To measure the timing, we can first call it, then it builds the graph. So we basically remove the graph building time. If something is called multiple times, usually we are interested in the successive calls time, not just the first call.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zfit</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">set_mode</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># test first in eager mode</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it -n1 -r5
<span class="nb">print</span><span class="p">(</span><span class="n">sum_func</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1651598754519.6846, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1676086221630.883, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1620098532209.491, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1573940576682.322, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1630701905322.774, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>28.5 ms ± 495 µs per loop (mean ± std. dev. of 5 runs, 1 loop each)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>This gives us a reference. As we see, the values are different each time, as expected. Now let’s run with the graph mode on. As mentioned, the first call just measures the graph building time + one single execution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zfit</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">set_mode_default</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;zfit.util.temporary.TemporarilySet at 0x7f0a3843ee80&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it -n1 -r1
<span class="nb">print</span><span class="p">(</span><span class="n">sum_func</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1617797395512.902, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>222 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>This takes significantly more time then the eager execution. Now we can execute it and measure the time of the succesive calls</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it -n1 -r5  # 5 repetitions
<span class="nb">print</span><span class="p">(</span><span class="n">sum_func</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1611353738480.6372, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1634789211925.7605, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1671613871337.332, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1623924152936.9832, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(1600701306881.366, shape=(), dtype=float64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10.3 ms ± 608 µs per loop (mean ± std. dev. of 5 runs, 1 loop each)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>That’s a significant speedup! It is clear that for a few evaluations, it does not matter <em>too much</em>. But this is about the scalability: imagine we have a large fit, where a minimizer needs hundreds or thousands of evaluations: that’s when the initial Graph building becomes neglectible and the speedup matters.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "zfit/poster",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./zfit-tutorials_copy/components"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By zfit<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>